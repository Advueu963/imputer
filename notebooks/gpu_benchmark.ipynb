{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c81a73",
   "metadata": {},
   "source": [
    "# GPU vs CPU Imputation Benchmark\n",
    "\n",
    "This notebook benchmarks GPU-accelerated imputation vs CPU imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f26a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '/home/student/Hackathon/imputer/src')\n",
    "from imputer.gpu_imputer import GPUImageImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b776eef",
   "metadata": {},
   "source": [
    "import torch\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '/home/student/Hackathon/imputer/src')\n",
    "from imputer.gpu_imputer import GPUImageImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b138e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Check availability\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd62b67",
   "metadata": {},
   "source": [
    "## Single Image Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d800f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.0218s\n",
      "GPU time: 0.1750s\n",
      "Speedup: 0.12x\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/student/Hackathon/imputer/girafe.jpg\"\n",
    "\n",
    "# CPU\n",
    "cpu_imputer = GPUImageImputer(device=\"cpu\")\n",
    "start = time.time()\n",
    "cpu_pred = cpu_imputer.impute_single(image_path)\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_imputer = GPUImageImputer(device=\"cuda\")\n",
    "    start = time.time()\n",
    "    gpu_pred = gpu_imputer.impute_single(image_path)\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"CPU time: {cpu_time:.4f}s\")\n",
    "    print(f\"GPU time: {gpu_time:.4f}s\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(f\"CPU time: {cpu_time:.4f}s\")\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062cd0e",
   "metadata": {},
   "source": [
    "## Batch Processing Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91102b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU batch time: 0.6033s (12.07ms per image)\n",
      "GPU batch time: 0.2174s (4.35ms per image)\n",
      "Speedup: 2.77x\n"
     ]
    }
   ],
   "source": [
    "# Simulate batch of 50 images\n",
    "batch_paths = [image_path] * 50\n",
    "\n",
    "# CPU\n",
    "cpu_imputer = GPUImageImputer(device=\"cpu\", batch_size=16)\n",
    "start = time.time()\n",
    "cpu_batch = cpu_imputer.impute_batch(batch_paths)\n",
    "cpu_batch_time = time.time() - start\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_imputer = GPUImageImputer(device=\"cuda\", batch_size=16)\n",
    "    start = time.time()\n",
    "    gpu_batch = gpu_imputer.impute_batch(batch_paths)\n",
    "    gpu_batch_time = time.time() - start\n",
    "    \n",
    "    print(f\"CPU batch time: {cpu_batch_time:.4f}s ({cpu_batch_time/50*1000:.2f}ms per image)\")\n",
    "    print(f\"GPU batch time: {gpu_batch_time:.4f}s ({gpu_batch_time/50*1000:.2f}ms per image)\")\n",
    "    print(f\"Speedup: {cpu_batch_time/gpu_batch_time:.2f}x\")\n",
    "else:\n",
    "    print(f\"CPU batch time: {cpu_batch_time:.4f}s ({cpu_batch_time/50*1000:.2f}ms per image)\")\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ea531",
   "metadata": {},
   "source": [
    "## Masked Tensor Imputation Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37aedd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU tensor time: 0.0188s\n",
      "GPU tensor time: 0.0157s\n",
      "Speedup: 1.20x\n"
     ]
    }
   ],
   "source": [
    "# Large tensor batch\n",
    "tensor = torch.randn(32, 3, 224, 224)\n",
    "mask = torch.randint(0, 2, (32, 3, 224, 224))\n",
    "\n",
    "# CPU\n",
    "cpu_imputer = GPUImageImputer(device=\"cpu\")\n",
    "start = time.time()\n",
    "cpu_imputed = cpu_imputer.impute_masked_tensor(tensor, mask)\n",
    "cpu_tensor_time = time.time() - start\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_imputer = GPUImageImputer(device=\"cuda\")\n",
    "    start = time.time()\n",
    "    gpu_imputed = gpu_imputer.impute_masked_tensor(tensor, mask)\n",
    "    gpu_tensor_time = time.time() - start\n",
    "    \n",
    "    print(f\"CPU tensor time: {cpu_tensor_time:.4f}s\")\n",
    "    print(f\"GPU tensor time: {gpu_tensor_time:.4f}s\")\n",
    "    print(f\"Speedup: {cpu_tensor_time/gpu_tensor_time:.2f}x\")\n",
    "else:\n",
    "    print(f\"CPU tensor time: {cpu_tensor_time:.4f}s\")\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec79f6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "GPU acceleration provides significant speedup for:\n",
    "- Single image inference\n",
    "- Batch processing (scales better)\n",
    "- Large tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb44d1",
   "metadata": {},
   "source": [
    "## Baseline Imputer Strategies Benchmark\n",
    "\n",
    "Test all baseline strategies (mean, median, constant) with GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d2dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all baseline strategies on GPU vs CPU:\n",
      "\n",
      "--- MEAN Strategy ---\n",
      "CPU: 0.0886s, Top prob: 1.0000\n",
      "GPU: 0.0171s, Top prob: 1.0000\n",
      "Speedup: 5.18x\n",
      "\n",
      "--- MEDIAN Strategy ---\n",
      "CPU: 0.0852s, Top prob: 1.0000\n",
      "GPU: 0.0168s, Top prob: 1.0000\n",
      "Speedup: 5.08x\n",
      "\n",
      "--- CONSTANT Strategy ---\n",
      "CPU: 0.0841s, Top prob: 1.0000\n",
      "GPU: 0.0028s, Top prob: 1.0000\n",
      "Speedup: 30.09x\n",
      "\n",
      "CPU: 0.0852s, Top prob: 1.0000\n",
      "GPU: 0.0168s, Top prob: 1.0000\n",
      "Speedup: 5.08x\n",
      "\n",
      "--- CONSTANT Strategy ---\n",
      "CPU: 0.0841s, Top prob: 1.0000\n",
      "GPU: 0.0028s, Top prob: 1.0000\n",
      "Speedup: 30.09x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from imputer.gpu_imputer import GPUBaselineImputer\n",
    "\n",
    "# Load multiple images as background data\n",
    "image_path = \"/home/student/Hackathon/imputer/girafe.jpg\"\n",
    "data_cpu = torch.stack([read_image(image_path).float() for _ in range(10)])\n",
    "data_gpu = data_cpu.clone()\n",
    "\n",
    "# Test image\n",
    "test_img = read_image(image_path).float().unsqueeze(0)\n",
    "\n",
    "print(\"Testing all baseline strategies on GPU vs CPU:\\n\")\n",
    "\n",
    "for strategy in [\"mean\", \"median\", \"constant\"]:\n",
    "    print(f\"--- {strategy.upper()} Strategy ---\")\n",
    "    \n",
    "    # CPU\n",
    "    cpu_baseline = GPUBaselineImputer(\n",
    "        model=cpu_imputer.model,\n",
    "        data=data_cpu,\n",
    "        device=\"cpu\",\n",
    "        strategy=strategy,  # type: ignore\n",
    "        constant_value=128.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    coalition = torch.tensor([True, False, True])\n",
    "    cpu_imputed = cpu_baseline.impute_with_coalition(test_img, coalition)\n",
    "    cpu_preds = cpu_baseline.predict_batch(cpu_imputed)\n",
    "    cpu_strategy_time = time.time() - start\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_baseline = GPUBaselineImputer(\n",
    "            model=gpu_imputer.model,\n",
    "            data=data_gpu,\n",
    "            device=\"cuda\",\n",
    "            strategy=strategy,  # type: ignore\n",
    "            constant_value=128.0\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        gpu_imputed = gpu_baseline.impute_with_coalition(test_img, coalition)\n",
    "        gpu_preds = gpu_baseline.predict_batch(gpu_imputed)\n",
    "        gpu_strategy_time = time.time() - start\n",
    "        \n",
    "        print(f\"CPU: {cpu_strategy_time:.4f}s, Top prob: {cpu_preds[0].max():.4f}\")\n",
    "        print(f\"GPU: {gpu_strategy_time:.4f}s, Top prob: {gpu_preds[0].max():.4f}\")\n",
    "        print(f\"Speedup: {cpu_strategy_time/gpu_strategy_time:.2f}x\\n\")\n",
    "    else:\n",
    "        print(f\"CPU: {cpu_strategy_time:.4f}s, Top prob: {cpu_preds[0].max():.4f}\")\n",
    "        print(\"GPU not available\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbbc0f",
   "metadata": {},
   "source": [
    "## Large Dataset Test\n",
    "\n",
    "Process multiple images with different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0090c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([100, 3, 614, 1024])\n",
      "\n",
      "=== Batch size: 16 ===\n",
      "GPU processed 100 images in 0.3894s\n",
      "Throughput: 256.77 images/sec\n",
      "\n",
      "=== Batch size: 32 ===\n",
      "GPU processed 100 images in 0.3894s\n",
      "Throughput: 256.77 images/sec\n",
      "\n",
      "=== Batch size: 32 ===\n",
      "GPU processed 100 images in 0.3585s\n",
      "Throughput: 278.93 images/sec\n",
      "\n",
      "=== Batch size: 64 ===\n",
      "GPU processed 100 images in 0.3585s\n",
      "Throughput: 278.93 images/sec\n",
      "\n",
      "=== Batch size: 64 ===\n",
      "GPU processed 100 images in 0.3662s\n",
      "Throughput: 273.09 images/sec\n",
      "GPU processed 100 images in 0.3662s\n",
      "Throughput: 273.09 images/sec\n"
     ]
    }
   ],
   "source": [
    "# Create dataset of 100 images\n",
    "large_dataset = torch.stack([read_image(image_path).float() for _ in range(100)])\n",
    "print(f\"Dataset shape: {large_dataset.shape}\")\n",
    "\n",
    "# Background data (20 images)\n",
    "background = large_dataset[:20]\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n=== Batch size: {bs} ===\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_bl = GPUBaselineImputer(\n",
    "            model=gpu_imputer.model,\n",
    "            data=background,\n",
    "            device=\"cuda\",\n",
    "            batch_size=bs,\n",
    "            strategy=\"mean\"  # type: ignore\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(0, len(large_dataset), bs):\n",
    "            batch = large_dataset[i:i+bs]\n",
    "            coalition = torch.ones(3, dtype=torch.bool)\n",
    "            imputed = gpu_bl.impute_with_coalition(batch, coalition)\n",
    "            preds = gpu_bl.predict_batch(imputed)\n",
    "        gpu_large_time = time.time() - start\n",
    "        \n",
    "        print(f\"GPU processed 100 images in {gpu_large_time:.4f}s\")\n",
    "        print(f\"Throughput: {100/gpu_large_time:.2f} images/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8c9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imputer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
